{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdd4fc2",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from scipy.signal import decimate\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06975798",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The KAIST Dataset (Jung2022) has 4 sensed variables: acoustic, current, temperature, and vibration. In this case, only vibration will be used.\n",
    "- **Machine Conditions (Normal, BPFI, BPFO, Misalign, and Unbalance):** Fault types and normal operating condition\n",
    "- **Motor load (0Nm, 2Nm, 4Nm):** Torque applied to the motor simulating load\n",
    "- **Fault Severity:** Depending on the type of fault there are at least 3 severities. For example BPFI has a fault severity represented in the crack width (0.3mm, 1mm, 3mm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b95fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading domain 0...\n",
      "  Loading 0Nm_Normal from .mat file...\n",
      "  Saving 0Nm_Normal to CSV...\n",
      "  Loading 0Nm_BPFI_03 from .mat file...\n",
      "  Saving 0Nm_BPFI_03 to CSV...\n",
      "  Loading 0Nm_BPFO_03 from .mat file...\n",
      "  Saving 0Nm_BPFO_03 to CSV...\n",
      "  Loading 0Nm_Misalign_01 from .mat file...\n",
      "  Saving 0Nm_Misalign_01 to CSV...\n",
      "  Loading 0Nm_Unbalance_0583mg from .mat file...\n",
      "  Saving 0Nm_Unbalance_0583mg to CSV...\n",
      "Domain 0 loaded: 5 classes\n",
      "Loading domain 1...\n",
      "  Loading 2Nm_Normal from .mat file...\n",
      "  Saving 2Nm_Normal to CSV...\n",
      "  Loading 2Nm_BPFI_03 from .mat file...\n",
      "  Saving 2Nm_BPFI_03 to CSV...\n",
      "  Loading 2Nm_BPFO_03 from .mat file...\n",
      "  Saving 2Nm_BPFO_03 to CSV...\n",
      "  Loading 2Nm_Misalign_01 from .mat file...\n",
      "  Saving 2Nm_Misalign_01 to CSV...\n",
      "  Loading 2Nm_Unbalance_0583mg from .mat file...\n",
      "  Saving 2Nm_Unbalance_0583mg to CSV...\n",
      "Domain 1 loaded: 5 classes\n",
      "Loading domain 2...\n",
      "  Loading 4Nm_Normal from .mat file...\n",
      "  Saving 4Nm_Normal to CSV...\n",
      "  Loading 4Nm_BPFI_03 from .mat file...\n",
      "  Saving 4Nm_BPFI_03 to CSV...\n",
      "  Loading 4Nm_BPFO_03 from .mat file...\n",
      "  Saving 4Nm_BPFO_03 to CSV...\n",
      "  Loading 4Nm_Misalign_01 from .mat file...\n",
      "  Saving 4Nm_Misalign_01 to CSV...\n",
      "  Loading 4Nm_Unbalance_0583mg from .mat file...\n",
      "  Saving 4Nm_Unbalance_0583mg to CSV...\n",
      "Domain 2 loaded: 5 classes\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ KAIST Dataset ------------------------------\n",
    "# Paths and parameters\n",
    "vibration_mat_folder_kaist = '../../data/raw_kaist/vibration_mat_25.6kHz'\n",
    "v_length_kaist = 1536000  # length of the smaller vector in the dataset\n",
    "sampling_rate_kaist = 25600  # fixed at 25.6 kHz\n",
    "\n",
    "# Class labels for different domains\n",
    "class_labels_kaist0 = ['0Nm_Normal', '0Nm_BPFI_03', '0Nm_BPFO_03', '0Nm_Misalign_01', '0Nm_Unbalance_0583mg']\n",
    "class_labels_kaist1 = ['2Nm_Normal', '2Nm_BPFI_03', '2Nm_BPFO_03', '2Nm_Misalign_01', '2Nm_Unbalance_0583mg']\n",
    "class_labels_kaist2 = ['4Nm_Normal', '4Nm_BPFI_03', '4Nm_BPFO_03', '4Nm_Misalign_01', '4Nm_Unbalance_0583mg']\n",
    "\n",
    "def load_domain_data_kaist(class_labels, domain_name):\n",
    "    \"\"\"Load vibration data for a domain, using CSV cache if available.\"\"\"\n",
    "    vibration_data = {}\n",
    "\n",
    "    folder_path = f'../../data/flex-data/vibration{sampling_rate_kaist/1000}kHz_kaist_domain{domain_name}'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading domain {domain_name}...\")\n",
    "\n",
    "    for label in class_labels:\n",
    "        csv_path = f'{folder_path}/{label}.csv'\n",
    "        mat_path = os.path.join(vibration_mat_folder_kaist, f\"{label}.mat\")\n",
    "\n",
    "        # Try CSV first, fall back to .mat\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"  Loading {label} from CSV...\")\n",
    "            vibration_data[label] = pd.read_csv(csv_path).values\n",
    "        elif os.path.exists(mat_path):\n",
    "            print(f\"  Loading {label} from .mat file...\")\n",
    "            mat_data = sio.loadmat(mat_path)\n",
    "            vibration = mat_data['Signal']['y_values'][0][0][0][0][0][:v_length_kaist, :]\n",
    "            vibration_data[label] = vibration\n",
    "\n",
    "            # Save to CSV for next time\n",
    "            print(f\"  Saving {label} to CSV...\")\n",
    "            pd.DataFrame(vibration_data[label]).to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"  Warning: {label} not found\")\n",
    "\n",
    "    print(f\"Domain {domain_name} loaded: {len(vibration_data)} classes\")\n",
    "    return vibration_data\n",
    "\n",
    "# Load all domains\n",
    "vibration_data_kaist0 = load_domain_data_kaist(class_labels_kaist0, '0Nm')\n",
    "vibration_data_kaist1 = load_domain_data_kaist(class_labels_kaist1, '2Nm')\n",
    "vibration_data_kaist2 = load_domain_data_kaist(class_labels_kaist2, '4Nm')\n",
    "\n",
    "vibration_data_kaist = {**vibration_data_kaist0, **vibration_data_kaist1, **vibration_data_kaist2}\n",
    "class_labels_kaist = list(vibration_data_kaist.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ CWRU Dataset ------------------------------\n",
    "# Paths and parameters\n",
    "vibration_mat_folder_cwru = '../../data/raw_cwru/vibration_mat'\n",
    "v_length_cwru = 1536000 # Lenght of the smaller vector in the dataset\n",
    "decimation_factor_cwru = 1\n",
    "sampling_rate_cwru = 12000\n",
    "\n",
    "# Class labels for different domains\n",
    "class_labels_cwru0 = ['0Nm_Normal', '0Nm_BPFI_03', '0Nm_BPFO_03', '0Nm_Misalign_01', '0Nm_Unbalance_0583mg']\n",
    "class_labels_cwru1 = ['2Nm_Normal', '2Nm_BPFI_03', '2Nm_BPFO_03', '2Nm_Misalign_01', '2Nm_Unbalance_0583mg']\n",
    "class_labels_cwru2 = ['4Nm_Normal', '4Nm_BPFI_03', '4Nm_BPFO_03', '4Nm_Misalign_01', '4Nm_Unbalance_0583mg']\n",
    "\n",
    "def load_domain_data_cwru(class_labels, domain_name):\n",
    "    \"\"\"Load vibration data for a domain, using CSV cache if available.\"\"\"\n",
    "    vibration_data = {}\n",
    "    folder_path = f'../../data/flex-data/vibration{int(25600/decimation_factor_cwru)/1000}kHz_cwru_domain{domain_name}'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading domain {domain_name}...\")\n",
    "    \n",
    "    for label in class_labels:\n",
    "        csv_path = f'{folder_path}/{label}.csv'\n",
    "        mat_path = os.path.join(vibration_mat_folder, f\"{label}.mat\")\n",
    "        \n",
    "        # Try CSV first, fall back to .mat\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"  Loading {label} from CSV...\")\n",
    "            vibration_data[label] = pd.read_csv(csv_path).values\n",
    "        elif os.path.exists(mat_path):\n",
    "            print(f\"  Loading {label} from .mat file...\")\n",
    "            mat_data = sio.loadmat(mat_path)\n",
    "            vibration = mat_data['Signal']['y_values'][0][0][0][0][0][:v_length, :]\n",
    "            vibration_data[label] = decimate(vibration[:v_length,:], decimation_factor_cwru, axis=0)\n",
    "            \n",
    "            # Save to CSV for next time\n",
    "            print(f\"  Saving {label} to CSV...\")\n",
    "            pd.DataFrame(vibration_data[label]).to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"  Warning: {label} not found\")\n",
    "    \n",
    "    print(f\"Domain {domain_name} loaded: {len(vibration_data)} classes\")\n",
    "    return vibration_data\n",
    "\n",
    "# Load all domains\n",
    "vibration_data_cwru0 = load_domain_data_cwru(class_labels_cwru0, '0')\n",
    "vibration_data_cwru1 = load_domain_data_cwru(class_labels_cwru1, '1')\n",
    "vibration_data_cwru2 = load_domain_data_cwru(class_labels_cwru2, '2')\n",
    "\n",
    "vibration_data_cwru = {**vibration_data_cwru0, **vibration_data_cwru1, **vibration_data_cwru2}\n",
    "class_labels_cwru = list(vibration_data_cwru.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e5a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.x\n",
    "# pip install scipy numpy pandas\n",
    "import json\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "# ------- config -------\n",
    "TARGET_FS = 25600  # Hz to match KAIST\n",
    "OUT_DIR = pathlib.Path(\"../../data/out\")\n",
    "\n",
    "# Helpers to find time-domain arrays in CWRU .mat\n",
    "CWRU_KEY_ORDER = [\n",
    "    # common CWRU naming patterns\n",
    "    r\".*_DE_time$\",   # drive end accel\n",
    "    r\".*_FE_time$\",   # fan end accel\n",
    "    r\".*_BA_time$\",   # base accel\n",
    "    r\".*_AE_time$\",   # acoustic emission if present\n",
    "    r\".*DE$\", r\".*FE$\", r\".*BA$\",\n",
    "    r\".*X\\d?$\", r\".*Y\\d?$\",    # add patterns for KAIST accelerometers\n",
    "]\n",
    "\n",
    "def _flatten_1d(x):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 2 and 1 in x.shape:\n",
    "        x = x.reshape(-1)\n",
    "    return x.squeeze()\n",
    "\n",
    "def _extract_channels(matdict):\n",
    "    # collect all 1D numeric arrays\n",
    "    candidates = {}\n",
    "    for k, v in matdict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        try:\n",
    "            arr = _flatten_1d(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if np.issubdtype(arr.dtype, np.number) and arr.ndim == 1 and arr.size > 10:\n",
    "            candidates[k] = arr\n",
    "\n",
    "    # order channels by regex priority\n",
    "    ordered = []\n",
    "    used = set()\n",
    "    for pat in CWRU_KEY_ORDER:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for k in list(candidates.keys()):\n",
    "            if k in used:\n",
    "                continue\n",
    "            if rx.match(k):\n",
    "                ordered.append(candidates[k])\n",
    "                used.add(k)\n",
    "    # add any leftover numeric vectors\n",
    "    for k, arr in candidates.items():\n",
    "        if k not in used:\n",
    "            ordered.append(arr)\n",
    "\n",
    "    return ordered[:4]  # up to four channels\n",
    "\n",
    "def _infer_fs_from_mat(matdict, default_fs=12000):\n",
    "    # try common field names\n",
    "    for k in matdict.keys():\n",
    "        if \"fs\" == k.lower() or \"samplingrate\" in k.lower():\n",
    "            fs_val = np.asarray(matdict[k]).astype(float).squeeze()\n",
    "            if fs_val.size >= 1:\n",
    "                return float(fs_val.flat[0])\n",
    "    # try to guess from CWRU naming like X097_DE_time -> 12 kHz or 48 kHz are common\n",
    "    return float(default_fs)\n",
    "\n",
    "def _stack_to_four(chans):\n",
    "    \"\"\"stack list of 1D arrays to shape [N,4], padding with zeros if fewer than 4.\"\"\"\n",
    "    maxlen = min([len(c) for c in chans]) if len(chans) > 0 else 0\n",
    "    if len(chans) == 0 or maxlen == 0:\n",
    "        return np.zeros((0, 4), dtype=np.float32)\n",
    "    # align to shortest to keep synchronous\n",
    "    chans = [c[:maxlen] for c in chans]\n",
    "    X = np.zeros((maxlen, 4), dtype=np.float32)\n",
    "    for i, c in enumerate(chans[:4]):\n",
    "        X[:, i] = c.astype(np.float32)\n",
    "    return X\n",
    "\n",
    "def _resample_if_needed(X, fs_in, fs_out):\n",
    "    fs_in = int(round(float(fs_in)))\n",
    "    fs_out = int(round(float(fs_out)))\n",
    "    if X.size == 0 or fs_in == fs_out:\n",
    "        return X\n",
    "    frac = Fraction(fs_out, fs_in)  # now both ints\n",
    "    up, down = frac.numerator, frac.denominator\n",
    "    Y = resample_poly(X, up, down, axis=0)\n",
    "    return Y.astype(np.float32)\n",
    "\n",
    "def convert_mat_to_csv(mat_path, label=\"Normal\", load_nm=None, dataset_id=\"cwru\",\n",
    "                       or_loc=None, out_dir=OUT_DIR):\n",
    "    mat_path = pathlib.Path(mat_path)\n",
    "    md = loadmat(mat_path.as_posix(), squeeze_me=False, struct_as_record=False)\n",
    "\n",
    "    chans = _extract_channels(md)\n",
    "    fs_in = _infer_fs_from_mat(md, default_fs=12000.0)\n",
    "    X = _stack_to_four(chans)\n",
    "    X = _resample_if_needed(X, fs_in, TARGET_FS)\n",
    "\n",
    "    # write CSV with no header, four columns: sensor1..sensor4\n",
    "    csv_name = mat_path.stem + \".csv\"\n",
    "    csv_path = out_dir / csv_name\n",
    "    np.savetxt(csv_path.as_posix(), X, delimiter=\",\", fmt=\"%.7f\")\n",
    "\n",
    "    # write sidecar metadata\n",
    "    meta = {\n",
    "        \"source_file\": mat_path.name,\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"label\": label,                 # e.g., normal, inner_race, ball, outer_race\n",
    "        \"outer_race_location\": or_loc,  # e.g., \"3\", \"6\", \"12\" oclock if known\n",
    "        \"load_Nm\": load_nm,             # if known\n",
    "        \"sampling_rate_hz\": TARGET_FS,\n",
    "        \"channels\": [\"sensor1\", \"sensor2\", \"sensor3\", \"sensor4\"],\n",
    "        \"notes\": \"headerless CSV for CNN. Values are vibration samples. Missing channels are zero padded.\",\n",
    "    }\n",
    "    json_path = out_dir / (mat_path.stem + \".json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return csv_path.as_posix(), json_path.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5f740",
   "metadata": {},
   "source": [
    "CWRU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac0cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/vibrationCWRU/OR007@12_0.csv',\n",
       " '../../data/vibrationCWRU/OR007@12_0.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cwru_dir = pathlib.Path(\"../../data/vibrationCWRU\")\n",
    "cwru_dir.mkdir(parents=True, exist_ok=True)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/IR007_0.mat\", label=\"inner_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/B007_0.mat\", label=\"ball\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/Normal_0.mat\", label=\"normal\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@3_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@6_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@12_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634594ca",
   "metadata": {},
   "source": [
    "KAIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4137c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/vibrationKAIST/0Nm_Normal.csv',\n",
       " '../../data/vibrationKAIST/0Nm_Normal.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- config -------\n",
    "kaist_dir = pathlib.Path(\"../../data/vibrationKAIST\")\n",
    "kaist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_BPFI_03.mat\", label=\"inner_race\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_BPFO_03.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Misalign_01.mat\", label=\"misalignment\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Unbalance_0583mg.mat\", label=\"unbalance\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Normal.mat\", label=\"normal\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa19f8",
   "metadata": {},
   "source": [
    "Manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82e4e689",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m         meta[\u001b[33m\"\u001b[39m\u001b[33mouter_race_location\u001b[39m\u001b[33m\"\u001b[39m] = infer_or_location_from_name(meta[\u001b[33m\"\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# quick integrity check\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m sample = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sample.shape[\u001b[32m1\u001b[39m] == \u001b[32m4\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must have 4 columns\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isfinite(sample.values).all(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has non finite values\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import json, hashlib, pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your two dataset folders\n",
    "ROOTS = [\n",
    "    pathlib.Path(\"../../data/vibrationCWRU\"),\n",
    "    pathlib.Path(\"../../data/vibrationKAIST\")\n",
    "]\n",
    "\n",
    "MANIFEST_CSV = \"../../data/manifest.csv\"\n",
    "TRAIN_CSV = \"../../data/train_manifest.csv\"\n",
    "VAL_CSV   = \"../../data/val_manifest.csv\"\n",
    "TEST_CSV  = \"../../data/test_manifest.csv\"\n",
    "\n",
    "def infer_or_location_from_name(name: str):\n",
    "    n = name.lower()\n",
    "    if \"@3\" in n: return \"3\"\n",
    "    if \"@6\" in n: return \"6\"\n",
    "    if \"@12\" in n: return \"12\"\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for ROOT in ROOTS:\n",
    "    for json_path in sorted(ROOT.glob(\"*.json\")):\n",
    "        meta = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "        csv_path = ROOT / (pathlib.Path(meta[\"source_file\"]).stem + \".csv\")\n",
    "        if not csv_path.exists():\n",
    "            print(f\"skip no csv for {json_path.name}\")\n",
    "            continue\n",
    "\n",
    "        # fill missing outer race location for CWRU\n",
    "        if meta.get(\"dataset_id\") == \"cwru\" and meta.get(\"label\") == \"outer_race\":\n",
    "            if not meta.get(\"outer_race_location\"):\n",
    "                meta[\"outer_race_location\"] = infer_or_location_from_name(meta[\"source_file\"])\n",
    "\n",
    "        # quick integrity check\n",
    "        sample = pd.read_csv(csv_path, header=None, nrows=5)\n",
    "        assert sample.shape[1] == 4, f\"{csv_path} must have 4 columns\"\n",
    "        assert np.isfinite(sample.values).all(), f\"{csv_path} has non finite values\"\n",
    "\n",
    "        n_rows = sum(1 for _ in open(csv_path, \"r\", encoding=\"utf-8\"))\n",
    "        row = {\n",
    "            \"filepath\": str(csv_path.resolve()),\n",
    "            \"dataset_id\": meta.get(\"dataset_id\"),\n",
    "            \"label\": meta.get(\"label\"),\n",
    "            \"load_Nm\": meta.get(\"load_Nm\"),\n",
    "            \"sampling_rate_hz\": meta.get(\"sampling_rate_hz\"),\n",
    "            \"outer_race_location\": meta.get(\"outer_race_location\"),\n",
    "            \"n_rows\": n_rows,\n",
    "            \"n_channels\": 4,\n",
    "            \"_source_json\": json_path.name\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "manifest = pd.DataFrame(rows)\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(f\"wrote manifest with {len(manifest)} files\")\n",
    "\n",
    "# deterministic split\n",
    "def stable_bin(path: str):\n",
    "    h = hashlib.sha1(path.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) / 0xFFFFFFFF\n",
    "\n",
    "manifest = manifest.sample(frac=1.0, random_state=13)\n",
    "print(\"Columns in manifest:\", manifest.columns.tolist())\n",
    "print(manifest.head())\n",
    "bins = manifest[\"filepath\"].map(stable_bin)\n",
    "manifest_train = manifest[bins < 0.80]\n",
    "manifest_val   = manifest[(bins >= 0.80) & (bins < 0.90)]\n",
    "manifest_test  = manifest[bins >= 0.90]\n",
    "\n",
    "manifest_train.to_csv(TRAIN_CSV, index=False)\n",
    "manifest_val.to_csv(VAL_CSV, index=False)\n",
    "manifest_test.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(f\"train {len(manifest_train)}  val {len(manifest_val)}  test {len(manifest_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34d7add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Scanning folder: C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\data\\vibrationCWRU\n",
      "   Found 6 JSON files: ['B007_0.json', 'IR007_0.json', 'Normal_0.json', 'OR007@12_0.json', 'OR007@3_0.json', 'OR007@6_0.json']\n",
      "‚û°Ô∏è Processing B007_0.json\n",
      "   ‚úÖ B007_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: B007_0.csv ‚Üí exists=True\n",
      "‚û°Ô∏è Processing IR007_0.json\n",
      "   ‚úÖ IR007_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: IR007_0.csv ‚Üí exists=True\n",
      "‚û°Ô∏è Processing Normal_0.json\n",
      "   ‚úÖ Normal_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: Normal_0.csv ‚Üí exists=True\n",
      "‚û°Ô∏è Processing OR007@12_0.json\n",
      "   ‚úÖ OR007@12_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: OR007@12_0.csv ‚Üí exists=True\n",
      "‚û°Ô∏è Processing OR007@3_0.json\n",
      "   ‚úÖ OR007@3_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: OR007@3_0.csv ‚Üí exists=True\n",
      "‚û°Ô∏è Processing OR007@6_0.json\n",
      "   ‚úÖ OR007@6_0.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: OR007@6_0.csv ‚Üí exists=True\n",
      "\n",
      "üîé Scanning folder: C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framework\\data\\vibrationKAIST\n",
      "   Found 5 JSON files: ['0Nm_BPFI_03.json', '0Nm_BPFO_03.json', '0Nm_Misalign_01.json', '0Nm_Normal.json', '0Nm_Unbalance_0583mg.json']\n",
      "‚û°Ô∏è Processing 0Nm_BPFI_03.json\n",
      "   ‚úÖ 0Nm_BPFI_03.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: 0Nm_BPFI_03.csv ‚Üí exists=True\n",
      "   ‚ùå Integrity check failed for 0Nm_BPFI_03.csv: No columns to parse from file\n",
      "‚û°Ô∏è Processing 0Nm_BPFO_03.json\n",
      "   ‚úÖ 0Nm_BPFO_03.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: 0Nm_BPFO_03.csv ‚Üí exists=True\n",
      "   ‚ùå Integrity check failed for 0Nm_BPFO_03.csv: No columns to parse from file\n",
      "‚û°Ô∏è Processing 0Nm_Misalign_01.json\n",
      "   ‚úÖ 0Nm_Misalign_01.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: 0Nm_Misalign_01.csv ‚Üí exists=True\n",
      "   ‚ùå Integrity check failed for 0Nm_Misalign_01.csv: No columns to parse from file\n",
      "‚û°Ô∏è Processing 0Nm_Normal.json\n",
      "   ‚úÖ 0Nm_Normal.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: 0Nm_Normal.csv ‚Üí exists=True\n",
      "   ‚ùå Integrity check failed for 0Nm_Normal.csv: No columns to parse from file\n",
      "‚û°Ô∏è Processing 0Nm_Unbalance_0583mg.json\n",
      "   ‚úÖ 0Nm_Unbalance_0583mg.json is a dict with keys: ['source_file', 'dataset_id', 'label', 'outer_race_location', 'load_Nm', 'sampling_rate_hz', 'channels', 'notes']\n",
      "   Looking for CSV: 0Nm_Unbalance_0583mg.csv ‚Üí exists=True\n",
      "   ‚ùå Integrity check failed for 0Nm_Unbalance_0583mg.csv: No columns to parse from file\n",
      "\n",
      "‚úÖ wrote manifest with 6 files to ./manifest.csv\n",
      "Columns in manifest: ['filepath', 'dataset_id', 'label', 'load_Nm', 'sampling_rate_hz', 'outer_race_location', 'n_rows', 'n_channels', '_source_json']\n",
      "                                            filepath dataset_id       label  \\\n",
      "1  C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framewor...       cwru  inner_race   \n",
      "3  C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framewor...       cwru  outer_race   \n",
      "4  C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framewor...       cwru  outer_race   \n",
      "5  C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framewor...       cwru  outer_race   \n",
      "0  C:\\Users\\giljo\\Desktop\\Master\\FlexUNS-Framewor...       cwru        ball   \n",
      "\n",
      "   load_Nm  sampling_rate_hz outer_race_location  n_rows  n_channels  \\\n",
      "1        0             25600                None  258699           4   \n",
      "3        0             25600                  12  260867           4   \n",
      "4        0             25600                   3  260867           4   \n",
      "5        0             25600                   6  260248           4   \n",
      "0        0             25600                None  261485           4   \n",
      "\n",
      "      _source_json  \n",
      "1     IR007_0.json  \n",
      "3  OR007@12_0.json  \n",
      "4   OR007@3_0.json  \n",
      "5   OR007@6_0.json  \n",
      "0      B007_0.json  \n",
      "train 6  val 0  test 0\n"
     ]
    }
   ],
   "source": [
    "import json, hashlib, pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your two dataset folders\n",
    "ROOTS = [\n",
    "    pathlib.Path(\"../../data/vibrationCWRU\"),\n",
    "    pathlib.Path(\"../../data/vibrationKAIST\")\n",
    "]\n",
    "\n",
    "MANIFEST_CSV = \"./manifest.csv\"\n",
    "TRAIN_CSV = \"./train_manifest.csv\"\n",
    "VAL_CSV   = \"./val_manifest.csv\"\n",
    "TEST_CSV  = \"./test_manifest.csv\"\n",
    "\n",
    "def infer_or_location_from_name(name: str):\n",
    "    n = name.lower()\n",
    "    if \"@3\" in n: return \"3\"\n",
    "    if \"@6\" in n: return \"6\"\n",
    "    if \"@12\" in n: return \"12\"\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for ROOT in ROOTS:\n",
    "    print(f\"\\nüîé Scanning folder: {ROOT.resolve()}\")\n",
    "    json_files = list(ROOT.glob(\"*.json\"))\n",
    "    print(f\"   Found {len(json_files)} JSON files: {[f.name for f in json_files]}\")\n",
    "\n",
    "    for json_path in sorted(json_files):\n",
    "        print(f\"‚û°Ô∏è Processing {json_path.name}\")\n",
    "        try:\n",
    "            meta = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Could not load {json_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # If meta is a list, print size and first element\n",
    "        if isinstance(meta, list):\n",
    "            print(f\"   ‚ö†Ô∏è {json_path.name} contains a list with {len(meta)} entries (not a single dict)\")\n",
    "        elif isinstance(meta, dict):\n",
    "            print(f\"   ‚úÖ {json_path.name} is a dict with keys: {list(meta.keys())}\")\n",
    "\n",
    "        # If list, skip for now (because your code expects dict per file)\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "\n",
    "        csv_path = ROOT / (pathlib.Path(meta[\"source_file\"]).stem + \".csv\")\n",
    "        print(f\"   Looking for CSV: {csv_path.name} ‚Üí exists={csv_path.exists()}\")\n",
    "\n",
    "        if not csv_path.exists():\n",
    "            print(f\"   ‚ùå skip no csv for {json_path.name}\")\n",
    "            continue\n",
    "\n",
    "        # fill missing outer race location for CWRU\n",
    "        if meta.get(\"dataset_id\") == \"cwru\" and meta.get(\"label\") == \"outer_race\":\n",
    "            if not meta.get(\"outer_race_location\"):\n",
    "                meta[\"outer_race_location\"] = infer_or_location_from_name(meta[\"source_file\"])\n",
    "\n",
    "        # quick integrity check\n",
    "        try:\n",
    "            sample = pd.read_csv(csv_path, header=None, nrows=5)\n",
    "            assert sample.shape[1] == 4, f\"{csv_path} must have 4 columns\"\n",
    "            assert np.isfinite(sample.values).all(), f\"{csv_path} has non finite values\"\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Integrity check failed for {csv_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        n_rows = sum(1 for _ in open(csv_path, \"r\", encoding=\"utf-8\"))\n",
    "        row = {\n",
    "            \"filepath\": str(csv_path.resolve()),\n",
    "            \"dataset_id\": meta.get(\"dataset_id\"),\n",
    "            \"label\": meta.get(\"label\"),\n",
    "            \"load_Nm\": meta.get(\"load_Nm\"),\n",
    "            \"sampling_rate_hz\": meta.get(\"sampling_rate_hz\"),\n",
    "            \"outer_race_location\": meta.get(\"outer_race_location\"),\n",
    "            \"n_rows\": n_rows,\n",
    "            \"n_channels\": 4,\n",
    "            \"_source_json\": json_path.name\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "manifest = pd.DataFrame(rows)\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(f\"\\n‚úÖ wrote manifest with {len(manifest)} files to {MANIFEST_CSV}\")\n",
    "\n",
    "# deterministic split\n",
    "def stable_bin(path: str):\n",
    "    h = hashlib.sha1(path.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) / 0xFFFFFFFF\n",
    "\n",
    "if not manifest.empty:\n",
    "    manifest = manifest.sample(frac=1.0, random_state=13)\n",
    "    print(\"Columns in manifest:\", manifest.columns.tolist())\n",
    "    print(manifest.head())\n",
    "    bins = manifest[\"filepath\"].map(stable_bin)\n",
    "    manifest_train = manifest[bins < 0.80]\n",
    "    manifest_val   = manifest[(bins >= 0.80) & (bins < 0.90)]\n",
    "    manifest_test  = manifest[bins >= 0.90]\n",
    "\n",
    "    manifest_train.to_csv(TRAIN_CSV, index=False)\n",
    "    manifest_val.to_csv(VAL_CSV, index=False)\n",
    "    manifest_test.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "    print(f\"train {len(manifest_train)}  val {len(manifest_val)}  test {len(manifest_test)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Manifest is empty ‚Äî no rows collected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
