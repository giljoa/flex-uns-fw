{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e5a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.x\n",
    "# pip install scipy numpy pandas\n",
    "import json\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "# ------- config -------\n",
    "TARGET_FS = 25600  # Hz to match KAIST\n",
    "OUT_DIR = pathlib.Path(\"../../data/out\")\n",
    "\n",
    "# Helpers to find time-domain arrays in CWRU .mat\n",
    "CWRU_KEY_ORDER = [\n",
    "    # common CWRU naming patterns\n",
    "    r\".*_DE_time$\",   # drive end accel\n",
    "    r\".*_FE_time$\",   # fan end accel\n",
    "    r\".*_BA_time$\",   # base accel\n",
    "    r\".*_AE_time$\",   # acoustic emission if present\n",
    "    r\".*DE$\", r\".*FE$\", r\".*BA$\",\n",
    "    r\".*X\\d?$\", r\".*Y\\d?$\",    # add patterns for KAIST accelerometers\n",
    "]\n",
    "\n",
    "def _flatten_1d(x):\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 2 and 1 in x.shape:\n",
    "        x = x.reshape(-1)\n",
    "    return x.squeeze()\n",
    "\n",
    "def _extract_channels(matdict):\n",
    "    # collect all 1D numeric arrays\n",
    "    candidates = {}\n",
    "    for k, v in matdict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        try:\n",
    "            arr = _flatten_1d(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if np.issubdtype(arr.dtype, np.number) and arr.ndim == 1 and arr.size > 10:\n",
    "            candidates[k] = arr\n",
    "\n",
    "    # order channels by regex priority\n",
    "    ordered = []\n",
    "    used = set()\n",
    "    for pat in CWRU_KEY_ORDER:\n",
    "        rx = re.compile(pat, re.IGNORECASE)\n",
    "        for k in list(candidates.keys()):\n",
    "            if k in used:\n",
    "                continue\n",
    "            if rx.match(k):\n",
    "                ordered.append(candidates[k])\n",
    "                used.add(k)\n",
    "    # add any leftover numeric vectors\n",
    "    for k, arr in candidates.items():\n",
    "        if k not in used:\n",
    "            ordered.append(arr)\n",
    "\n",
    "    return ordered[:4]  # up to four channels\n",
    "\n",
    "def _infer_fs_from_mat(matdict, default_fs=12000):\n",
    "    # try common field names\n",
    "    for k in matdict.keys():\n",
    "        if \"fs\" == k.lower() or \"samplingrate\" in k.lower():\n",
    "            fs_val = np.asarray(matdict[k]).astype(float).squeeze()\n",
    "            if fs_val.size >= 1:\n",
    "                return float(fs_val.flat[0])\n",
    "    # try to guess from CWRU naming like X097_DE_time -> 12 kHz or 48 kHz are common\n",
    "    return float(default_fs)\n",
    "\n",
    "def _stack_to_four(chans):\n",
    "    \"\"\"stack list of 1D arrays to shape [N,4], padding with zeros if fewer than 4.\"\"\"\n",
    "    maxlen = min([len(c) for c in chans]) if len(chans) > 0 else 0\n",
    "    if len(chans) == 0 or maxlen == 0:\n",
    "        return np.zeros((0, 4), dtype=np.float32)\n",
    "    # align to shortest to keep synchronous\n",
    "    chans = [c[:maxlen] for c in chans]\n",
    "    X = np.zeros((maxlen, 4), dtype=np.float32)\n",
    "    for i, c in enumerate(chans[:4]):\n",
    "        X[:, i] = c.astype(np.float32)\n",
    "    return X\n",
    "\n",
    "def _resample_if_needed(X, fs_in, fs_out):\n",
    "    fs_in = int(round(float(fs_in)))\n",
    "    fs_out = int(round(float(fs_out)))\n",
    "    if X.size == 0 or fs_in == fs_out:\n",
    "        return X\n",
    "    frac = Fraction(fs_out, fs_in)  # now both ints\n",
    "    up, down = frac.numerator, frac.denominator\n",
    "    Y = resample_poly(X, up, down, axis=0)\n",
    "    return Y.astype(np.float32)\n",
    "\n",
    "def convert_mat_to_csv(mat_path, label=\"Normal\", load_nm=None, dataset_id=\"cwru\",\n",
    "                       or_loc=None, out_dir=OUT_DIR):\n",
    "    mat_path = pathlib.Path(mat_path)\n",
    "    md = loadmat(mat_path.as_posix(), squeeze_me=False, struct_as_record=False)\n",
    "\n",
    "    chans = _extract_channels(md)\n",
    "    fs_in = _infer_fs_from_mat(md, default_fs=12000.0)\n",
    "    X = _stack_to_four(chans)\n",
    "    X = _resample_if_needed(X, fs_in, TARGET_FS)\n",
    "\n",
    "    # write CSV with no header, four columns: sensor1..sensor4\n",
    "    csv_name = mat_path.stem + \".csv\"\n",
    "    csv_path = out_dir / csv_name\n",
    "    np.savetxt(csv_path.as_posix(), X, delimiter=\",\", fmt=\"%.7f\")\n",
    "\n",
    "    # write sidecar metadata\n",
    "    meta = {\n",
    "        \"source_file\": mat_path.name,\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"label\": label,                 # e.g., normal, inner_race, ball, outer_race\n",
    "        \"outer_race_location\": or_loc,  # e.g., \"3\", \"6\", \"12\" oclock if known\n",
    "        \"load_Nm\": load_nm,             # if known\n",
    "        \"sampling_rate_hz\": TARGET_FS,\n",
    "        \"channels\": [\"sensor1\", \"sensor2\", \"sensor3\", \"sensor4\"],\n",
    "        \"notes\": \"headerless CSV for CNN. Values are vibration samples. Missing channels are zero padded.\",\n",
    "    }\n",
    "    json_path = out_dir / (mat_path.stem + \".json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return csv_path.as_posix(), json_path.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5f740",
   "metadata": {},
   "source": [
    "CWRU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac0cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/vibrationCWRU/OR007@12_0.csv',\n",
       " '../../data/vibrationCWRU/OR007@12_0.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cwru_dir = pathlib.Path(\"../../data/vibrationCWRU\")\n",
    "cwru_dir.mkdir(parents=True, exist_ok=True)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/IR007_0.mat\", label=\"inner_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/B007_0.mat\", label=\"ball\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/Normal_0.mat\", label=\"normal\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@3_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@6_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_cwru/vibration_mat/OR007@12_0.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"cwru\", out_dir=cwru_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634594ca",
   "metadata": {},
   "source": [
    "KAIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4137c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/vibrationKAIST/0Nm_Normal.csv',\n",
       " '../../data/vibrationKAIST/0Nm_Normal.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- config -------\n",
    "kaist_dir = pathlib.Path(\"../../data/vibrationKAIST\")\n",
    "kaist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_BPFI_03.mat\", label=\"inner_race\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_BPFO_03.mat\", label=\"outer_race\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Misalign_01.mat\", label=\"misalignment\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Unbalance_0583mg.mat\", label=\"unbalance\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)\n",
    "convert_mat_to_csv(\"../../data/raw_kaist/vibration_mat/0Nm_Normal.mat\", label=\"normal\", load_nm=0, dataset_id=\"kaist\", out_dir=kaist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e580a",
   "metadata": {},
   "source": [
    "Join JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f3c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged 7 JSON files into ..\\..\\data\\vibrationCWRU\\cwru_metadata.json and deleted originals.\n",
      "✅ Merged 6 JSON files into ..\\..\\data\\vibrationKAIST\\kaist_metadata.json and deleted originals.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../../data/vibrationKAIST/kaist_metadata.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "def join_jsons(folder, out_file=\"merged.json\"):\n",
    "    folder = pathlib.Path(folder)\n",
    "    json_files = sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "    all_data = []\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                # keep track of original file\n",
    "                if isinstance(data, dict):\n",
    "                    data[\"_source_file\"] = jf.name\n",
    "                all_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {jf}: {e}\")\n",
    "\n",
    "    out_path = folder / out_file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "\n",
    "    # delete original JSON files except the merged one\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            if jf.name != out_file:\n",
    "                jf.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not delete {jf}: {e}\")\n",
    "\n",
    "    print(f\"✅ Merged {len(all_data)} JSON files into {out_path} and deleted originals.\")\n",
    "    return out_path\n",
    "\n",
    "# Merge all JSONs in ./out_csv into merged.json\n",
    "join_jsons(cwru_dir, out_file=\"cwru_metadata.json\")\n",
    "join_jsons(kaist_dir, out_file=\"kaist_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa19f8",
   "metadata": {},
   "source": [
    "Manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e4e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote manifest with 0 files\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(h[:\u001b[38;5;241m8\u001b[39m], \u001b[38;5;241m16\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0xFFFFFFFF\u001b[39m\n\u001b[0;32m     65\u001b[0m manifest \u001b[38;5;241m=\u001b[39m manifest\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[43mmanifest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilepath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(stable_bin)\n\u001b[0;32m     67\u001b[0m manifest_train \u001b[38;5;241m=\u001b[39m manifest[bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.80\u001b[39m]\n\u001b[0;32m     68\u001b[0m manifest_val   \u001b[38;5;241m=\u001b[39m manifest[(bins \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.80\u001b[39m) \u001b[38;5;241m&\u001b[39m (bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.90\u001b[39m)]\n",
      "File \u001b[1;32mw:\\Git\\flex-uns-fw\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mw:\\Git\\flex-uns-fw\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'filepath'"
     ]
    }
   ],
   "source": [
    "import json, hashlib, pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your two dataset folders\n",
    "ROOTS = [\n",
    "    pathlib.Path(\"./data/vibrationCWRU\"),\n",
    "    pathlib.Path(\"./data/vibrationKAIST\")\n",
    "]\n",
    "\n",
    "MANIFEST_CSV = \"./manifest.csv\"\n",
    "TRAIN_CSV = \"./train_manifest.csv\"\n",
    "VAL_CSV   = \"./val_manifest.csv\"\n",
    "TEST_CSV  = \"./test_manifest.csv\"\n",
    "\n",
    "def infer_or_location_from_name(name: str):\n",
    "    n = name.lower()\n",
    "    if \"@3\" in n: return \"3\"\n",
    "    if \"@6\" in n: return \"6\"\n",
    "    if \"@12\" in n: return \"12\"\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for ROOT in ROOTS:\n",
    "    for json_path in sorted(ROOT.glob(\"*.json\")):\n",
    "        meta = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "        csv_path = ROOT / (pathlib.Path(meta[\"source_file\"]).stem + \".csv\")\n",
    "        if not csv_path.exists():\n",
    "            print(f\"skip no csv for {json_path.name}\")\n",
    "            continue\n",
    "\n",
    "        # fill missing outer race location for CWRU\n",
    "        if meta.get(\"dataset_id\") == \"cwru\" and meta.get(\"label\") == \"outer_race\":\n",
    "            if not meta.get(\"outer_race_location\"):\n",
    "                meta[\"outer_race_location\"] = infer_or_location_from_name(meta[\"source_file\"])\n",
    "\n",
    "        # quick integrity check\n",
    "        sample = pd.read_csv(csv_path, header=None, nrows=5)\n",
    "        assert sample.shape[1] == 4, f\"{csv_path} must have 4 columns\"\n",
    "        assert np.isfinite(sample.values).all(), f\"{csv_path} has non finite values\"\n",
    "\n",
    "        n_rows = sum(1 for _ in open(csv_path, \"r\", encoding=\"utf-8\"))\n",
    "        row = {\n",
    "            \"filepath\": str(csv_path.resolve()),\n",
    "            \"dataset_id\": meta.get(\"dataset_id\"),\n",
    "            \"label\": meta.get(\"label\"),\n",
    "            \"load_Nm\": meta.get(\"load_Nm\"),\n",
    "            \"sampling_rate_hz\": meta.get(\"sampling_rate_hz\"),\n",
    "            \"outer_race_location\": meta.get(\"outer_race_location\"),\n",
    "            \"n_rows\": n_rows,\n",
    "            \"n_channels\": 4,\n",
    "            \"_source_json\": json_path.name\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "manifest = pd.DataFrame(rows)\n",
    "manifest.to_csv(MANIFEST_CSV, index=False)\n",
    "print(f\"wrote manifest with {len(manifest)} files\")\n",
    "\n",
    "# deterministic split\n",
    "def stable_bin(path: str):\n",
    "    h = hashlib.sha1(path.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) / 0xFFFFFFFF\n",
    "\n",
    "manifest = manifest.sample(frac=1.0, random_state=13)\n",
    "bins = manifest[\"filepath\"].map(stable_bin)\n",
    "manifest_train = manifest[bins < 0.80]\n",
    "manifest_val   = manifest[(bins >= 0.80) & (bins < 0.90)]\n",
    "manifest_test  = manifest[bins >= 0.90]\n",
    "\n",
    "manifest_train.to_csv(TRAIN_CSV, index=False)\n",
    "manifest_val.to_csv(VAL_CSV, index=False)\n",
    "manifest_test.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(f\"train {len(manifest_train)}  val {len(manifest_val)}  test {len(manifest_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
